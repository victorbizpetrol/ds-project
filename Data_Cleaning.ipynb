{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# import missingno as msno  ##pip install missingno\n",
    "\n",
    "pd.set_option('display.max_rows', 500) # your numbers here\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Read File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-024539ae9936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnull_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/car_v2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "null_df = pd.read_csv('Data/car_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initial count of Rows and Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Are there duplicate rows?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Fed RSSD' uniquely identifies each bank.  There are no duplicate banks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df['Fed RSSD'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df.duplicated().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there are duplicates in columns 'namehcr and 'Bank Name'. Below is more info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df['namehcr'].value_counts().head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df['Bank Name'].value_counts().head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df([null_df['Fed RSSD'].value_counts()>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df[null_df.duplicated('Fed RSSD')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df[null_df['namehcr']== 'WINTRUST FINANCIAL CORPORATION'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#null_df[null_df.duplicated(['Bank Name'], keep=False)]  # this code will display all duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df[null_df['Bank Name']== 'First State Bank'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Count of null at the Column level **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 127 rows and some rows with null values. Many of these null rows have a count of only nine nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df.isnull().sum().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#null_df.isnull().sum()\n",
    "null_df.isnull().sum().sort_values(ascending=False).head(45)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alternative: \n",
    "\n",
    "Let's find out how many missing data do we have.\n",
    "\n",
    "First, let's count the number of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = null_df.isnull().sum().sort_values(ascending=False)\n",
    "# Then, let's calculate the percentage of missing data per feature\n",
    "percent = (null_df.isnull().sum()/null_df.isnull().count()).sort_values(ascending=False)*100\n",
    "# Finally, let's concatenate Total and Percent into another dataframe\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **More Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Find a pattern: Are there several columns with null values at the same index rows?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Retreive the row number for the solitary row which has \"Number Employees\" as null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns=null_df.columns[null_df.isnull().any()] \n",
    "# Step one - Assign null_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(null_df[null_df[\"Number Employees\"].isnull()][null_columns])\n",
    "\n",
    "# Step two - \n",
    "#Are there other columns with null value at the same index than sample column \"Number Employees\"\n",
    "#The answer is Yes. It appears to be several colums with nine null value at the same index as seen in the results below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing null values at indexes = 268, 282, 313, 390, 3127, 3194, 4175, 4176, 4380"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing null value at index row= 282\n",
    "null_df.loc[282]['Number Employees']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_null_step1= null_df[null_df['Number Employees'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_null_step1.shape\n",
    "# AS seenm, we removed a total of nine rows. Our dataset shape is now 5542 rowns (previously 5551 rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have removed most of the columns with null value count = 9 and 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in code below, we still have several null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_null_step1.isnull().sum().sort_values(ascending=False).head(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this step, we will drop those columns containing more than 80% of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df = filtered_null_step1.columns[filtered_null_step1.isna().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns containing nulls - At this point, we just visualizing all columns with null values\n",
    "null_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns containing >80% of nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtered_null_step2 = filtered_null_step1.dropna(thresh=500, axis=1)  #Keep only the rows with at least 500 non-na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_null_step3 = filtered_null_step1.dropna(thresh=0.8*len(filtered_null_step1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_null_step2 = filtered_null_step1.dropna(thresh=0.8*len(filtered_null_step1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_null_step2 = filtered_null_step1.loc[:, filtered_null_step1.isnull().sum() < 0.8*filtered_null_step1.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have dropped ccolumns accountting with 80% of null values.\n",
    "\n",
    "Let's see how many columns with null values we still have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_null_step2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_null_step2.isnull().sum().sort_values(ascending=False).head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a visualization matrix very interesarting  from library \"missingno\" but it doesn't really work well with a large datasets.\n",
    "\n",
    "#msno.matrix(filtered_null_step2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill out Columns containing nulls > 500 with its mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_null_step2[\"Credit Loss Prov to Chargeoffs\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_null_step2[\"Loan Loss Allow to noncurr Loans\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_null_step2.shape\n",
    "# As seen, we removed a total of five columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are going to segment the banks by 'Total Assets' so that we may replace nulls with segment mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small <$50M, Medium = $50M to $50B, Large = $50B to $3T\n",
    "filtered_null_step2['Bank Size'] = pd.cut(filtered_null_step2['Total Assets'], [0,50000, 50000000, 3000000000], \n",
    "labels=['Small', 'Medium', 'Large'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Create a list of columns that are type float or int.  We will iterate over these columns and replace nulls \n",
    "with the mean value for the column based on banks segments small, medium and large.\n",
    "\n",
    "Step 2: create a mask for banks that have null values in 'col_name' and are bank size\n",
    "    \n",
    "Step 3: get mean value for the col_name based on bank size.\n",
    "\n",
    "Step 4: replace nulls with mean value for bank size segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [col_name for col_name, dtype in filtered_null_step2.dtypes.iteritems() if (dtype == np.float or dtype == np.int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in columns:\n",
    "    if filtered_null_step2[col_name].isnull().any():\n",
    "        #Replace nulls for small banks\n",
    "        mask = (filtered_null_step2[col_name].isnull()) & (filtered_null_step2['Bank Size'] == 'Small')\n",
    "        segment_mean = filtered_null_step2.loc[filtered_null_step2['Bank Size'] == 'Small', col_name].mean()\n",
    "        filtered_null_step2.loc[mask, [col_name]] = segment_mean\n",
    "        \n",
    "        #Replace nulls for medium banks\n",
    "        mask = (filtered_null_step2[col_name].isnull()) & (filtered_null_step2['Bank Size'] == 'Medium')\n",
    "        segment_mean = filtered_null_step2.loc[filtered_null_step2['Bank Size'] == 'Medium', col_name].mean()\n",
    "        filtered_null_step2.loc[mask, [col_name]] = segment_mean\n",
    "        \n",
    "        #Replace nulls for large banks\n",
    "        mask = (filtered_null_step2[col_name].isnull()) & (filtered_null_step2['Bank Size'] == 'Large')\n",
    "        segment_mean = filtered_null_step2.loc[filtered_null_step2['Bank Size'] == 'Large', col_name].mean()\n",
    "        filtered_null_step2.loc[mask, [col_name]] = segment_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_null_step2.isnull().sum().sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_null_step2.to_csv('Data/car_v3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gonzalo's code for the last piece \n",
    "# What are we going to do with this code as far as presentation purposes are concerned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- First, I reassigned our daaset to a different name so that i can test my code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gon_df = filtered_null_step3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- We set our segments using the \"cut method\" into 3 bins as seen in code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small <$50M, Medium = $50M to $50B, Large = $50B to $3T\n",
    "gon_df['Bank Size'] = pd.cut(gon_df['Total Assets'], [0,50000, 50000000, 3000000000], \n",
    "labels=['Small', 'Medium', 'Large'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Create a list of columns that are type float or int.  We will iterate over these columns and replace nulls \n",
    "with the mean value for the column based on banks segments small, medium and large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [col_name for col_name, dtype in gon_df.dtypes.iteritems() if (dtype == np.float or dtype == np.int)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dasaset in three pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_df = gon_df.loc[gon_df['Bank Size'] == 'Small']\n",
    "md_df = gon_df.loc[gon_df['Bank Size'] == 'Medium']\n",
    "lg_df = gon_df.loc[gon_df['Bank Size'] == 'Large']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check their shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's replace any null value with its average accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in columns:\n",
    "    sm_df[col_name].fillna(sm_df[col_name].mean(), inplace = True)\n",
    "    md_df[col_name].fillna(md_df[col_name].mean(), inplace = True)\n",
    "    lg_df[col_name].fillna(lg_df[col_name].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if there still any null value left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_df.loc[sm_df['Bank Size'] == 'Small'].isnull().sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_df.isnull().sum().sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lg_df.isnull().sum().sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will concatenate the three Dataframes together into one Dataframe called gon_df_conc and finally, we will create a cvs file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df1, df2, df3]\n",
    "\n",
    "In [5]: result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames =[sm_df, md_df, lg_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gon_df_conc = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gon_df_conc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gon_df_conc.to_csv('Data/car_v4.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
